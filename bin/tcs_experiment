import argparse
from tc_study.experiment.downstream_task_evaluation import compute_downstream_task, \
    compute_truncated_downstream_task
from tc_study.experiment.unsupervised_evaluation import compute_truncated_unsupervised_metrics, \
    compute_normalized_unsupervised_metrics
from tc_study.experiment.unsupervised_evaluation_mp import compute_normalized_unsupervised_metrics_mp, \
    compute_truncated_unsupervised_metrics_mp
from tc_study.experiment.passive_variables import compute_passive_variable_indexes
from tc_study.experiment.synthetic_unsupervised_evaluation import compare_metrics
from tc_study.experiment.passive_variables_mp import compute_passive_variable_indexes_mp


def compute_passive_variables(args):
    if args.multiprocess is not None:
        compute_passive_variable_indexes_mp(args.model_path, args.overwrite, nb_proc=args.multiprocess)
    else:
        compute_passive_variable_indexes(args.model_path, args.overwrite)


def compute_unsupervised_metrics(args):
    if args.multiprocess is not None:
        compute_unsupervised_metrics_mp(args)
    else:
        compute_unsupervised_metrics_sp(args)


def compute_unsupervised_metrics_sp(args):
    if args.truncate is True:
        compute_truncated_unsupervised_metrics(args.model_path, args.representation, overwrite=args.overwrite)
    else:
        compute_normalized_unsupervised_metrics(args.model_path, args.representation, overwrite=args.overwrite)


def compute_unsupervised_metrics_mp(args):
    if args.truncate is True:
        compute_truncated_unsupervised_metrics_mp(args.model_path, args.representation, overwrite=args.overwrite,
                                                  nb_proc=args.multiprocess)
    else:
        compute_normalized_unsupervised_metrics_mp(args.model_path, args.representation, overwrite=args.overwrite,
                                                   nb_proc=args.multiprocess)


def compute_downstream_tasks(args):
    if args.truncate is True:
        compute_truncated_downstream_task(args.model_path, args.representation, predictor=args.predictor,
                                          overwrite=args.overwrite)
    else:
        compute_downstream_task(args.model_path, args.representation, predictor=args.predictor,
                                overwrite=args.overwrite)


def compute_synthetic_eval(args):
    if args.size < args.active_variables or args.active_variables < 0 or args.size <= 0:
        raise ValueError("The number of active variable must be lower or equal to the total number of variables and "
                         "both values must be positive.")
    if args.noise < 0:
        raise ValueError("Noise must be positive")
    cov_min, cov_max = sorted(args.cov)
    compare_metrics(args.size, args.active_variables, cov_min=cov_min, cov_max=cov_max, noise_strength=args.noise)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Reproduce truncation experiment using dislib pre-trained VAE models")
    subparsers = parser.add_subparsers()

    pv = subparsers.add_parser("passive_variables", aliases=["pv", "variables"])
    pv.add_argument("model_path", type=str, help="Path where the pre-trained models are stored")
    pv.add_argument("--overwrite", "-o", action="store_true", help="Overwrite existing results")
    pv.add_argument("--multiprocess", "-m", nargs='?', const=4, type=int, help="UNSTABLE: Performs distributed computation")
    pv.set_defaults(func=compute_passive_variables)

    mc = subparsers.add_parser("metrics_comparison", aliases=["mc"])
    mc.add_argument("size", type=int, help="Dimensionality of the latent representation")
    mc.add_argument("active_variables", type=int, help="Number of active variables")
    mc.add_argument("--cov", "-c", type=float, nargs=2, help="Covariance range between active variables")
    mc.add_argument("--noise", "-n", type=float, help="Noise strength for active variables sampling")
    mc.set_defaults(func=compute_synthetic_eval)

    um = subparsers.add_parser("unsupervised_metrics", aliases=["um"])
    um.add_argument("model_path", type=str, help="Path where the pre-trained models are stored")
    um.add_argument("representation", type=str, choices=["mean", "sampled"],
                    help="Representation from which we need to compute unsupervised scores")
    um.add_argument("--truncate", "-t", action="store_true",
                    help="Compute truncated unsupervised metrics. Passive variables must be computed beforehand.")
    um.add_argument("--overwrite", "-o", action="store_true", help="Overwrite existing results")
    um.add_argument("--multiprocess", "-m", nargs='?', const=4, type=int, help="UNSTABLE: Performs distributed computation")
    um.set_defaults(func=compute_unsupervised_metrics)

    dt = subparsers.add_parser("downstream_tasks", aliases=["dt", "tasks"])
    dt.add_argument("model_path", type=str, help="Path where the pre-trained models are stored")
    dt.add_argument("representation", type=str, choices=["mean", "sampled"],
                    help="Representation which will be used for downstream tasks")
    dt.add_argument("predictor", type=str, choices=["logistic_regression_cv", "gradient_boosting_classifier"],
                    help="Predictor function to use")
    dt.add_argument("--truncate", "-t", action="store_true",
                    help="Compute truncated downstream tasks. Passive variables must be computed beforehand.")
    dt.add_argument("--overwrite", "-o", action="store_true", help="Overwrite existing results")
    dt.set_defaults(func=compute_downstream_tasks)

    res = parser.parse_args()
    res.func(res)
